{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6bb0a3",
   "metadata": {},
   "source": [
    "## Dataset3 Sentiment Analysis YouTube Comments (Step2)\n",
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe66ef9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T02:49:46.932248Z",
     "start_time": "2023-05-05T02:49:45.883041Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbcbe662",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T02:49:46.936099Z",
     "start_time": "2023-05-05T02:49:46.933659Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_comments(fileName):\n",
    "    text =  open(fileName,  encoding=\"utf8\")\n",
    "    line_count = sum(1 for line in open(fileName, encoding=\"utf8\"))\n",
    "\n",
    "    if \"train\" in fileName: \n",
    "        text = text.read() # read the text in the file\n",
    "        checking = BeautifulSoup(text, 'lxml')  # pulling data out of HTML decoding\n",
    "        text = checking.get_text()\n",
    "    print (\"File Type: \", type(text)) # return the type of content in the file str\n",
    "    return text, line_count # return the whole content and number of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d08c4ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T02:49:46.941546Z",
     "start_time": "2023-05-05T02:49:46.937331Z"
    }
   },
   "outputs": [],
   "source": [
    "def emojis(comment):\n",
    "    # Smile emojis\n",
    "    comment = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' emojipositive ', comment)\n",
    "    # Laugh emojis\n",
    "    comment = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' emojipositive ', comment)\n",
    "    # Love emojis\n",
    "    comment = re.sub(r'(<3|:\\*)', ' emojipositive ', comment)\n",
    "    # Wink emojis\n",
    "    comment = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' emojipositive ', comment)\n",
    "    # Sad emojis\n",
    "    comment = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' emojinegative ', comment)\n",
    "    # Cry emojis\n",
    "    comment = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' emojinegative ', comment)\n",
    "    \n",
    "    return comment\n",
    "\n",
    "\n",
    "def clean_words(stringL):\n",
    "    \n",
    "    comment = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', '', stringL)# urls removed\n",
    "    comment = re.sub(r'@(\\S+)', '', comment)      # @ removed\n",
    "    comment = re.sub(r'#(\\S+)', '', comment)      # hashtags removed\n",
    "    comment = re.sub('\\n', ' ', comment)            # more preprocessing\n",
    "    comment = re.sub(\"\\d+\", \" \", comment)           # replace numbers\n",
    "    comment = re.sub(r'(.)\\1+', r'\\1\\1', comment)   # more than two repeating charaters removed\n",
    "    comment = re.sub(r'\\s+', ' ', comment)          # replace additional whitespace\n",
    "    comment = emojis(comment)                       # calls the emojis function\n",
    "    \n",
    "    tr = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    comment = comment.translate(tr)   # remove punctuation from string\n",
    "    \n",
    "    text = comment.lower().split() # convert all tweets to lower-case\n",
    "   \n",
    "    \"\"\"\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    words = [word for word in text if word not in stop_words]   #removes stopwords that exist\n",
    "    \"\"\"\n",
    "    # lemmatise: grouping together the different inflected forms of a word\n",
    "    # so they can be analyzed as a single item.\n",
    "    lmtzr = WordNetLemmatizer() \n",
    "    wordlist = [lmtzr.lemmatize(x) for x in text] # lemmatization\n",
    "    \"\"\"\n",
    "    porter = PorterStemmer()\n",
    "    wordlist2 = [porter.stem(x) for x in text]    # stemmer can be added if needed\n",
    "    \n",
    "    snowball = SnowballStemmer('english')\n",
    "    wordlist3 = [snowball.stem(x) for x in words]\n",
    "    \"\"\"\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccc06623",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T02:49:47.599749Z",
     "start_time": "2023-05-05T02:49:46.943009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'very', 'exciting', 'okk']\n",
      "['it', 'is', 'too', 'bad', 'll']\n",
      "['i', 'am', 'going', 'to', 'college', 'emojipositive']\n"
     ]
    }
   ],
   "source": [
    "print (clean_words(\"It is very exciting okkkkkkkk www.in.ie @oo  #opoi\"))\n",
    "print (clean_words(\"It is toooooo bad\\!        ll\"))\n",
    "print (clean_words(\"I am going to colleges :-)  \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9063815c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T02:49:47.603277Z",
     "start_time": "2023-05-05T02:49:47.600880Z"
    }
   },
   "outputs": [],
   "source": [
    "def dictionary_probabilities(vocabulary, text):\n",
    "    wordcount = len(text)\n",
    "    # creates a dictionary from the given sequence of keys and values.\n",
    "    # initialze vocabulary to be the keys of the dictionary\n",
    "    dictionary = dict.fromkeys(vocabulary,0) \n",
    "    \n",
    "    for words in text:\n",
    "        dictionary[words]=dictionary[words]+1 # calculate frequency of words in list\n",
    "    \n",
    "    for key in dictionary:\n",
    "        #calculate the conditional log propbabilities using laplace smoothing\n",
    "        dictionary[key]= log((dictionary[key] + 1) / (wordcount + len(vocabulary))) \n",
    "        \n",
    "    return dictionary, wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dff9a4cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T02:49:47.608112Z",
     "start_time": "2023-05-05T02:49:47.604274Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_accuracy(fileName, vocabulary, textNeg, textPos, priorNeg, priorPos): \n",
    "    \n",
    "    neg_D, wordcount_Neg = dictionary_probabilities(vocabulary, textNeg)\n",
    "    pos_D, wordcount_Pos = dictionary_probabilities(vocabulary, textPos)\n",
    "\n",
    "    # load the testing file in\n",
    "    fileObj =  open(fileName, encoding=\"utf8\")\n",
    "    # counts how many comments there are in file\n",
    "    total_comments = sum(1 for line in open(fileName, encoding=\"utf8\"))     \n",
    "    text = fileObj.readlines()\n",
    "    print (\"Num test comment = \", len(text))\n",
    "    \n",
    "    correct = 0\n",
    "    name = 0\n",
    "    \n",
    "    for line in text:\n",
    "        checking = BeautifulSoup(line, 'lxml')    # HTML decoding    \n",
    "        line = checking.get_text().lower().split()\n",
    "\n",
    "        text = \" \".join(line)\n",
    "        comment = clean_words(text) # clean the testing data\n",
    "\n",
    "        sumNeg = 0\n",
    "        sumPos = 0\n",
    "        for word in comment:\n",
    "            if word in vocabulary:\n",
    "                sumNeg += neg_D[word]    # add the log conditional probability when found\n",
    "                sumPos += pos_D[word]\n",
    "\n",
    "           \n",
    "                \n",
    "        sumNeg += priorNeg   # add the prior probability\n",
    "        sumPos += priorPos\n",
    "    \n",
    "        if \"Neg\" in fileName:\n",
    "            if sumNeg > sumPos:     \n",
    "                correct +=1         # counting the no of correct predictions \n",
    "                name = \"negative\" \n",
    "        else:\n",
    "            if sumPos > sumNeg:\n",
    "                correct +=1\n",
    "                name = \"positive\"\n",
    "\n",
    "    print(\"The accuracy for\", name, \"testing comments is:\", round(((correct/total_comments)*100),2), \"%\") # print the accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a35347a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T02:53:54.481050Z",
     "start_time": "2023-05-05T02:49:47.608953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Type:  <class 'str'>\n",
      "Finished with neg train  419545\n",
      "File Type:  <class 'str'>\n",
      "Finished with pos train 1309125\n",
      "contentsNeg 913481\n",
      "contentsPos 175136\n",
      "Vocab 35949\n",
      "Starting test\n",
      "Num test comment =  208\n",
      "The accuracy for negative testing comments is: 92.31 %\n",
      "Num test comment =  158\n",
      "The accuracy for negative testing comments is: 92.41 %\n",
      "Neg test finished\n",
      "Num test comment =  882\n",
      "The accuracy for positive testing comments is: 71.88 %\n",
      "Num test comment =  658\n",
      "The accuracy for positive testing comments is: 71.58 %\n",
      "Pos test finished\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    # Load the positive and negative training documents \n",
    "    textNeg, total_neg_comments = load_comments(\"train_neg.txt\")\n",
    "    print (\"Finished with neg train \", total_neg_comments)\n",
    "    \n",
    "    textPos, total_pos_comments = load_comments(\"train_pos.txt\")\n",
    "    print (\"Finished with pos train\", total_pos_comments)\n",
    "    \n",
    "       \n",
    "    # Apply cleaning techniques and return positive and negative lists of words \n",
    "    contentsNeg = clean_words(textNeg)\n",
    "    contentsPos = clean_words(textPos)\n",
    "    \n",
    "    print(\"contentsNeg\", len(contentsNeg))\n",
    "    print(\"contentsPos\", len(contentsPos))\n",
    "\n",
    "    \n",
    "    # Create a vocabulary from combining sets of positive and negative words\n",
    "    vocab = set(contentsNeg) | set(contentsPos)\n",
    "    print(\"Vocab\", len(vocab))\n",
    "    \n",
    "    # Calculate the prior probability for both classes\n",
    "    priorNeg = log(total_neg_comments/(total_neg_comments+total_pos_comments))\n",
    "    priorPos = log(total_pos_comments/(total_neg_comments+total_pos_comments))\n",
    "    \n",
    "    print (\"Starting test\")\n",
    "    # Pass pieces of information needed to classify unseen documents\n",
    "    test_accuracy(\"test_Neg.txt\", vocab, contentsNeg, contentsPos, priorNeg, priorPos)\n",
    "    test_accuracy(\"test2_Neg.txt\", vocab, contentsNeg, contentsPos, priorNeg, priorPos)\n",
    "    print (\"Neg test finished\")\n",
    "    \n",
    "    test_accuracy(\"test_Pos.txt\", vocab, contentsNeg, contentsPos, priorNeg, priorPos)\n",
    "    test_accuracy(\"test2_Pos.txt\", vocab, contentsNeg, contentsPos, priorNeg, priorPos)\n",
    "    print (\"Pos test finished\")\n",
    "    \n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d441c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
